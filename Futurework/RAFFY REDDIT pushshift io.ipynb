{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def get_date(created):\n",
    "    # get the date of post\n",
    "    return dt.date.fromtimestamp(created)\n",
    "\n",
    "\n",
    "\n",
    "def query_pushshift(subreddit, kind='submission', skip=5, times=50, \n",
    "                    subfield = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'num_comments',\n",
    "                                'score', 'is_self'],\n",
    "                    comfields = ['body', 'score', 'created_utc']):\n",
    "    \n",
    "    \n",
    "    # get the base url that contains information I want to scrape where 'kind' are all submitted posts\n",
    "    # and 'subreddit' is the specified subreddit. Get 500 posts.\n",
    "    stem = f\"https://api.pushshift.io/reddit/search/{kind}/?subreddit={subreddit}&size=500\"\n",
    "    \n",
    "    # instantiate list to contain \n",
    "    mylist = []\n",
    "   \n",
    "    # scrape posts from the subreddit 'times' times\n",
    "    for x in range(1, times + 1):\n",
    "        # Get posts 'skip' * 'x' days ago\n",
    "        URL = f\"{stem}&after={skip * x}d\"\n",
    "        print(URL)\n",
    "       \n",
    "        # Scrape URL\n",
    "        response = requests.get(URL)\n",
    "       \n",
    "        # Give me an AssertionError if status code not 200\n",
    "        assert response.status_code == 200\n",
    "       \n",
    "        # Of the HTML scraped, take the values of 'data'\n",
    "        the_json=response.json()\n",
    "        no_blanks=[c for c in the_json['data'] if ('selftext' in c.keys()) and len(c['selftext'])>10]\n",
    "        \n",
    "        # turn the data into a dataframe\n",
    "        df = pd.DataFrame.from_dict(no_blanks)\n",
    "        \n",
    "        # append the dataframe to mylist\n",
    "        mylist.append(df)\n",
    "        \n",
    "        # wait to not overrun Reddit's resources\n",
    "        time.sleep(3)\n",
    "   \n",
    "    # concatenate the dataframes together as one large dataframe, full\n",
    "    full = pd.concat(mylist, sort=False)\n",
    "    if kind == \"submission\":\n",
    "       \n",
    "        # take all speficied data\n",
    "        full = full[subfield]\n",
    "        \n",
    "        # drop duplicate rows\n",
    "        full = full.drop_duplicates()\n",
    "        full = full.loc[full['is_self'] == True]\n",
    "   \n",
    "    # date the the post was... posted\n",
    "    _timestamp = full[\"created_utc\"].apply(get_date)\n",
    "    full['timestamp'] = _timestamp\n",
    "    print(full.shape)\n",
    "    return full\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
